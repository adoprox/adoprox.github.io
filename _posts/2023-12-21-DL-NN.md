---
layout: post
title: "Image segmentation using DL"
description: null
image: assets/images/feature.jpg
nav-menu: true
---

<font size="6"><p>The following papers were used as a starting point to understand the nature of dataset. Moreover where the data is coming from and the purpose to automate its segmentation</p></font>

[Paper 1][PAPER 1] [Paper 2][PAPER 2]

[PAPER 1]: https://www.sciencedirect.com/science/article/abs/pii/S0378775318301459?via%3Dihub
[PAPER 2]: https://www.sciencedirect.com/science/article/abs/pii/S0378775317308182?via%3Dihub

<font size="6"><p>We technically used 2 datasets of the same nature i.e they were obtained using the same techniques. The key difference in the datasets were the spatial resolution, the number of classes and the availability of labelled data. The last difference is vital to building this solution as generating labelled tomographic X-ray data is labour-intensive and error prone. For the second dataset we manually labelled 4 images to conduct the training, validation and testing. </p></font>

<font size="6"><p>In this post we will go through the very basics and the thought process behind the work. More technical exploration can be done using my Github linked below. For this project we used 2 models VGGNet (figure 1) and UNET (figure 2). To set some context we need to understand how semantic segmentation works wrt NN, 2 processes are required for it to happen, namely feature extraction and then classification. In VGGNet feature extraction and classification is done separately, but in UNET both processes are part of one architecture. </p></font>

![VGGNet architecture]({{site.baseurl}}/assets/images/VGGNet.jpg)
**Figure 1: VGGNet architecture** 
![Unet architecute]({{site.baseurl}}/assets/images/unet_1.jpg)
**Figure 2: UNET architecture**
